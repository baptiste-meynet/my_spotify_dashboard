{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cda5821-b208-4c7e-8745-cb6d6d9818be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/baptistemeynet/Downloads/Projects/spotify_dashboard/Spotify Extended Streaming History/Streaming_History_Audio_2021-2022_5.json...\n",
      "Loading /Users/baptistemeynet/Downloads/Projects/spotify_dashboard/Spotify Extended Streaming History/Streaming_History_Audio_2019-2020_2.json...\n",
      "Loading /Users/baptistemeynet/Downloads/Projects/spotify_dashboard/Spotify Extended Streaming History/Streaming_History_Audio_2020-2021_4.json...\n",
      "Loading /Users/baptistemeynet/Downloads/Projects/spotify_dashboard/Spotify Extended Streaming History/Streaming_History_Audio_2022-2023_7.json...\n",
      "Loading /Users/baptistemeynet/Downloads/Projects/spotify_dashboard/Spotify Extended Streaming History/Streaming_History_Audio_2014-2018_0.json...\n",
      "Loading /Users/baptistemeynet/Downloads/Projects/spotify_dashboard/Spotify Extended Streaming History/Streaming_History_Video_2020-2024.json...\n",
      "Loading /Users/baptistemeynet/Downloads/Projects/spotify_dashboard/Spotify Extended Streaming History/Streaming_History_Audio_2020_3.json...\n",
      "Loading /Users/baptistemeynet/Downloads/Projects/spotify_dashboard/Spotify Extended Streaming History/Streaming_History_Audio_2023-2024_8.json...\n",
      "Loading /Users/baptistemeynet/Downloads/Projects/spotify_dashboard/Spotify Extended Streaming History/Streaming_History_Audio_2018-2019_1.json...\n",
      "Loading /Users/baptistemeynet/Downloads/Projects/spotify_dashboard/Spotify Extended Streaming History/Streaming_History_Audio_2022_6.json...\n",
      "Loading /Users/baptistemeynet/Downloads/Projects/spotify_dashboard/Spotify Extended Streaming History/Streaming_History_Audio_2024-2025_9.json...\n",
      "Dropped 1005 non-track rows (non-null media)\n",
      "Dropped 121 duplicate rows\n",
      "Dropped 0 rows missing track or artist\n",
      "\n",
      "Summary of row counts:\n",
      "  Initial:             148263\n",
      "  After media filter:  147258  (−1005)\n",
      "  After dedup:         147137  (−121)\n",
      "  After track/artist:  147137  (−0)\n",
      "\n",
      "Cleaned data written to: /Users/baptistemeynet/Downloads/Projects/spotify_dashboard/Spotify Extended Streaming History/combined_cleaned_streaming_history.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import pytz\n",
    "from pathlib import Path\n",
    "\n",
    "def convert_to_local(ts_utc, country_code):\n",
    "    \"\"\"Convert a UTC‐localized timestamp to the local timezone for a given country code.\"\"\"\n",
    "    try:\n",
    "        tz_name = pytz.country_timezones[country_code][0]\n",
    "    except (KeyError, IndexError):\n",
    "        tz_name = \"UTC\"\n",
    "    return ts_utc.tz_convert(tz_name)\n",
    "\n",
    "def main():\n",
    "    # 1. Discover files via glob\n",
    "    data_dir = Path(\"/Users/baptistemeynet/Downloads/Projects/spotify_dashboard/Spotify Extended Streaming History\")\n",
    "    json_files = glob.glob(str(data_dir / \"*.json\"))\n",
    "    \n",
    "    # 2. Load & concatenate\n",
    "    dfs = []\n",
    "    for fp in json_files:\n",
    "        print(f\"Loading {fp}...\")\n",
    "        dfs.append(pd.read_json(fp))  # assumes each JSON is a list of records\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Record initial row count\n",
    "    counts = {}\n",
    "    counts['initial'] = len(df)\n",
    "    \n",
    "     # 3. Filter out any row with a non-null value in any of these seven columns\n",
    "    drop_cols = [\n",
    "        \"episode_name\", \"episode_show_name\", \"spotify_episode_uri\",\n",
    "        \"audiobook_title\", \"audiobook_uri\",\n",
    "        \"audiobook_chapter_uri\", \"audiobook_chapter_title\"\n",
    "    ]\n",
    "    before = len(df)\n",
    "    mask_has_media = df[drop_cols].notnull().any(axis=1)\n",
    "    df = df[~mask_has_media]\n",
    "    counts['after_media_filter'] = len(df)\n",
    "    print(f\"Dropped {before - counts['after_media_filter']} non-track rows (non-null media)\")\n",
    "    \n",
    "    # 4. Drop the now-unneeded columns\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "    \n",
    "    # 5. Deduplicate (byte-for-byte across all columns)\n",
    "    before = len(df)\n",
    "    df.drop_duplicates(keep=\"first\", inplace=True)\n",
    "    counts['after_dedup'] = len(df)\n",
    "    print(f\"Dropped {before - counts['after_dedup']} duplicate rows\")\n",
    "    \n",
    "    # 6. Filter out rows with missing/blank track or artist\n",
    "    before = len(df)\n",
    "    track_blank = df['master_metadata_track_name'].fillna(\"\").eq(\"\")\n",
    "    artist_blank = df['master_metadata_album_artist_name'].fillna(\"\").eq(\"\")\n",
    "    df = df[~(track_blank | artist_blank)]\n",
    "    counts['after_track_artist_filter'] = len(df)\n",
    "    print(f\"Dropped {before - counts['after_track_artist_filter']} rows missing track or artist\")\n",
    "    \n",
    "    # 7. Timezone conversion\n",
    "    # Parse 'ts' to UTC-aware all at once:\n",
    "    df['ts_utc'] = pd.to_datetime(df['ts'], utc=True)\n",
    "\n",
    "    # Now convert each row into its country-local timezone:\n",
    "    df['ts_local'] = df.apply(\n",
    "        lambda row: convert_to_local(row['ts_utc'], row.get('conn_country', '')), axis=1\n",
    "    )\n",
    "    \n",
    "    # 8. Summary of removals\n",
    "    print(\"\\nSummary of row counts:\")\n",
    "    print(f\"  Initial:             {counts['initial']}\")\n",
    "    print(f\"  After media filter:  {counts['after_media_filter']}  (−{counts['initial'] - counts['after_media_filter']})\")\n",
    "    print(f\"  After dedup:         {counts['after_dedup']}  (−{counts['after_media_filter'] - counts['after_dedup']})\")\n",
    "    print(f\"  After track/artist:  {counts['after_track_artist_filter']}  (−{counts['after_dedup'] - counts['after_track_artist_filter']})\")\n",
    "    \n",
    "    # 9. Export to CSV\n",
    "    out_path = data_dir / \"combined_cleaned_streaming_history.csv\"\n",
    "    df.drop(columns=['ts_utc'], inplace=True)  # optional: drop the helper column\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"\\nCleaned data written to: {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becaead8-c0be-473e-bde8-f04ddc6d8f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
